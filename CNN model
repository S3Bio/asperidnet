import pandas as pd
import numpy as np
import torch
import torch.optim as optim
import torch.nn as nn
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import optuna
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from optuna.study import StudyDirection
from optuna._study_summary import StudySummary
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from matplotlib import cm
from cycler import cycler

class SpectralDataset:
    def __init__(self, filename, datafor='train'):
        data = pd.read_csv(filename)
        #data = data.sample(len(data)).reset_index(drop=True)
        #np.random.seed(0)
        cols =[x for x in data.columns if x not in ['target']]
        rowused = []
        for i in range (len(data)):
                if i % 10 == 0:
                    rowused.append('test')
                
                elif i % 10 == 1:
                    rowused.append('validate')
                
                else:
                    rowused.append('train')
                            
        data['rowused'] = rowused
        if datafor == 'train':
            selected_data = data.loc[data['rowused'] == 'train', :]
        elif datafor == 'validate':
            selected_data = data.loc[data['rowused'] == 'validate', :]
        elif datafor == 'test':
            selected_data = data.loc[data['rowused'] == 'test', :]
        elif datafor == 'all':
            selected_data = data
        else:
            raise("datafor supports only test train and validate")
        
        self.X = selected_data[cols]
        #print(self.X)
        le = LabelEncoder()
        data_y = selected_data.loc[:, 'target']
        self.y = le.fit_transform(data_y.values.ravel())
        self.y = self.y.reshape(-1)
        #le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
        #print(le_name_mapping)

        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, index):
        return (self.X.iloc[index, :].to_numpy(), self.y[index])


spectrum_train = SpectralDataset('df_newname.csv', datafor = 'train')
spectrum_test = SpectralDataset('df_newname.csv', datafor = 'test')
spectrum_val = SpectralDataset('df_newname.csv', datafor = 'validate')
    
train_loader = DataLoader(spectrum_train, batch_size=128, shuffle=True)
test_loader = DataLoader(spectrum_test, batch_size=64, shuffle=True)
valid_loader = DataLoader(spectrum_val, batch_size=64, shuffle=True)


class Model_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv1d(1,512, kernel_size=6, stride=3, padding=0),
            nn.BatchNorm1d(512),
            nn.ReLU())
            
        self.conv2 = nn.Sequential(
            nn.Conv1d(512, 512, kernel_size=6, stride=3),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=6, stride=2))        
        
        self.conv3 = nn.Sequential(
            nn.Conv1d(512, 256, kernel_size=6, stride=2),
            nn.BatchNorm1d(256),
            nn.ReLU())
       
        self.conv4 = nn.Sequential(
            nn.Conv1d(256, 256, kernel_size=4, stride=2),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=4, stride=2))
        
        self.conv5 = nn.Sequential(
            nn.Conv1d(256, 256, kernel_size=4, stride=1),
            nn.BatchNorm1d(256),
            nn.ReLU())
       
        self.conv6 = nn.Sequential(
            nn.Conv1d(256, 256, kernel_size=3, stride=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=3, stride=1))

        self.flatten = nn.Flatten()
        self.lin_1 = nn.Linear(256, 720)
        self.lin_2 = nn.Linear(720, 1576)
        self.lin_3 = nn.Linear(1576, 25)
        
    def forward(self, x):
        x = x.view(x.shape[0], 1,-1)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = self.conv6(x)
        x = self.flatten(x)
        x = F.relu(self.lin_1(x))
        x = F.relu(self.lin_2(x))
        x = self.lin_3(x)
        return x

model = Model_CNN()
optimizer = optim.Adam(model.parameters(), lr=1.462e-04)
criterion = nn.CrossEntropyLoss()

def train(model, optimizer, criterion, train_loader, valid_loader, test_loader, epochs=40, device='cpu'):
    val_accplot, val_lossplot = [], []
    train_accplot, train_lossplot = [], []
    best_loss = 1
    for epoch in range(epochs):
        model.train()
        training_loss = 0
        valid_loss = 0
        num_train_correct = 0
        num_train_examples = 0
        for batch in train_loader:
        #for inputs, target in train_loader:
            optimizer.zero_grad()
            inputs, target = batch
            inputs, target = inputs.float().to(device), target.to(device)
            target = target.type(torch.LongTensor)
            output = model(inputs)
            loss = criterion(output, target)
            #loss = criterion(output, target.view(-1))
            loss.backward()
            optimizer.step()
            training_loss += loss.data.item()
            predicted_train = torch.max(F.softmax(output, dim=1), dim=1)[1]
            #_, predicted_train = torch.max(F.softmax(output), dim=1)[1]
            train_correct = torch.eq(predicted_train, target).view(-1)
            num_train_correct += torch.sum(train_correct).item()
            num_train_examples += train_correct.shape[0]
        training_loss /= len(train_loader)
        train_accuracy = num_train_correct / num_train_examples
           
        model.eval()
        num_val_correct = 0
        num_val_examples = 0
        y_pred, y_true, = [], []
        with torch.no_grad():
            for batch in valid_loader:
            #for inputs, target in valid_loader:
                inputs, target = batch
                inputs, target = inputs.float().to(device), target.to(device)
                output = model(inputs)
                target = target.type(torch.LongTensor)
                loss = criterion(output, target)
                valid_loss += loss.data.item()
                y_score = F.softmax(output, dim=1)
                max_pred = torch.max(y_score, 1)[1]
                val_correct = torch.eq(max_pred,target).view(-1)
                num_val_correct += torch.sum(val_correct).item()
                num_val_examples += val_correct.shape[0]
                y_pred.append(y_score.cpu().detach().numpy())
                y_true.append(target.cpu().detach().numpy())
        valid_loss /= len(valid_loader)
        val_accuracy = num_val_correct / num_val_examples
        y_pred = np.concatenate(y_pred)
        y_true = np.concatenate(y_true)
        print('Epoch: {}, Training loss: {:.4f}, Valid loss= {:.4f}, Train accuracy = {:.4f}, Valid accuracy = {:.4f}'
              .format(epoch, training_loss, valid_loss, 100*train_accuracy, 100*val_accuracy))
        train_lossplot.append(training_loss)
        train_accplot.append(train_accuracy)
        val_accplot.append(val_accuracy)
        val_lossplot.append(valid_loss)

        if valid_loss < best_loss:
            print('Saving model...')
            best_loss = valid_loss
            torch.save(model.state_dict(), 'Best_model_07Dec.pth')
    
    #Plot model efficiency
    fig, ax = plt.subplots(nrows=2, ncols=1,figsize=(7,9))
    plt.tight_layout(pad=2, h_pad=6) #add space between edge and fig , 2 subplot 
    ax[0].plot(train_accplot, color='darkcyan', label="Train")
    ax[0].plot(val_accplot, color='darkorange', label="Test")
    ax[0].legend(fontsize=14,markerscale=1.5)
    ax[0].grid(True)
    ax[0].set_title("Training Accuracy", fontsize=16);
    ax[0].set_xlabel("Epoch",fontsize=14);
    ax[0].set_ylabel("Accuracy",fontsize=14);
    ax[1].plot(train_lossplot,  color='darkcyan', label="Train")
    ax[1].plot(val_lossplot, color='darkorange',label="Validation")
    ax[1].legend(fontsize=12,markerscale=1.5)
    ax[1].grid(True)
    ax[1].set_title("Training Loss", fontsize=16);
    ax[1].set_xlabel("Epoch",fontsize=14);
    ax[1].set_ylabel("Loss",fontsize=14);
    fig.savefig("Training_07Dec2021.png", format='png')
    
    #plot ROC at the last epoch
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(25):
        fpr[i], tpr[i], _ = roc_curve(y_true==i, y_pred[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    fig = plt.figure(figsize=(10,8))
    for i in range(25):
        plt.plot(fpr[i], tpr[i], lw=2,label='Class {0} (area = {1:0.2f})'''.format(i+1, roc_auc[i]))
    plt.plot([0, 1], [0, 1], color='slategrey', lw=1, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title('ROC curve', fontdict = {'fontsize' : 16})
    plt.legend(loc='lower right', prop={'size': 10})
    plt.show()
    fig.savefig('ROC_07Dec2021.png', format='png')


def test(device='cpu'): 
    model = Model_CNN()
    model.load_state_dict(torch.load('Best_model_07Dec.pth'))
    model.eval()
    num_correct = 0
    num_example = 0
    test_loss = 0
    with torch.no_grad():
        for batch in test_loader:
        #for inputs, target in test_loader:
            inputs, target = batch
            inputs, target = inputs.float().to(device), target.to(device)
            target = target.type(torch.LongTensor)
            output = model(inputs)
            loss = criterion(output, target)
            test_loss += loss.data.item()
            y_score_test = F.softmax(output, dim=1)
            max_pred_test = torch.max(y_score_test, 1)[1]
            #_, max_pred_test = torch.max(y_score_test, dim=1)[1]
            correct = torch.eq(max_pred_test,target).view(-1)
            num_correct += torch.sum(correct).item()
            num_example += correct.shape[0]
    test_loss /= len(test_loader)
    test_accuracy = 100 * num_correct / num_example
    print(f'Model performance is...Test loss: {test_loss}... Test Accuracy: {test_accuracy}')
    
        
train(model, optimizer, criterion, train_loader, valid_loader, test_loader)
print('-------------------------------------------------------------------------------------------------------------------') 
test()

print('-------------------------------------------------------------------------------------------------------------------')

print('Result of FT-IR MTEC 5 samples')

class FtirDataset:
    def __init__(self, filename):
        data = pd.read_csv(filename)
        cols =[x for x in data.columns if x not in ['target']]
        self.X = data[cols]
        self.y = data.loc[:, 'target']
        self.y = self.y.values.reshape(-1)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, index):
        return (self.X.iloc[index, :].to_numpy(), self.y[index])

spectrum_test_mtec = FtirDataset('df_ftir5.csv')
mtec_loader = DataLoader(spectrum_test_mtec, batch_size=1, shuffle=True)

def test_mtec(device='cpu'): 
    model = Model_CNN()
    model.load_state_dict(torch.load('Best_model_07Dec.pth'))
    criterion = nn.CrossEntropyLoss()
    model.eval()
    num_correct = 0
    num_example = 0
    test_loss = 0
    with torch.no_grad():
        for inputs, target in mtec_loader:
            inputs, target = inputs.float().to(device), target.to(device)
            target = target.type(torch.LongTensor)
            print('\ntarget:',target)
            output = model(inputs)
            loss = criterion(output, target)
            test_loss += loss.data.item()
            y_score_test = F.softmax(output, dim=1)
            max_pred_test = torch.max(y_score_test, 1)[1]
            print('Predict as:',max_pred_test)
            #_, max_pred_test = torch.max(y_score_test, dim=1)[1]
            correct = torch.eq(max_pred_test,target).view(-1)
            print('Accuracy:',correct)
            num_correct += torch.sum(correct).item()
            num_example += correct.shape[0]
    test_loss /= len(test_loader)
    test_accuracy = 100 * num_correct / num_example
    print(f'\nModel performance is : {test_accuracy} %')

test_mtec()
